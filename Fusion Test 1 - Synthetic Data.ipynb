{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "715b328e",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macOS\n",
      "Interactive plot activated\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import platform\n",
    "import random\n",
    "from tkinter import Tk\n",
    "\n",
    "from itertools import permutations\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, models,datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Extend width of Jupyter Notebook Cell to the size of browser\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "# OS related settings\n",
    "if platform.system() == 'Windows':\n",
    "    print('Windows')\n",
    "#     %matplotlib tk\n",
    "    %matplotlib qt\n",
    "elif platform.system() == 'Darwin':\n",
    "    print('macOS')\n",
    "    Tk().withdraw()\n",
    "    %matplotlib osx\n",
    "elif platform == 'linux' or platform == 'linux2':\n",
    "    print('Linux')\n",
    "# This line of \"print\" must exist right after %matplotlib command, otherwise JN will hang on the first import statement after this.\n",
    "print('Interactive plot activated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54248cff",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# ChINN\n",
    "\n",
    "# Convert decimal to binary string\n",
    "def sources_and_subsets_nodes(N):\n",
    "    str1 = \"{0:{fill}\"+str(N)+\"b}\"\n",
    "    a = []\n",
    "    for i in range(1,2**N):\n",
    "        a.append(str1.format(i, fill='0'))\n",
    "\n",
    "    sourcesInNode = []\n",
    "    sourcesNotInNode = []\n",
    "    subset = []\n",
    "    sourceList = list(range(N))\n",
    "    # find subset nodes of a node\n",
    "    def node_subset(node, sourcesInNodes):\n",
    "        return [node - 2**(i) for i in sourcesInNodes]\n",
    "    \n",
    "    # convert binary encoded string to integer list\n",
    "    def string_to_integer_array(s, ch):\n",
    "        N = len(s) \n",
    "        return [(N - i - 1) for i, ltr in enumerate(s) if ltr == ch]\n",
    "    \n",
    "    for j in range(len(a)):\n",
    "        # index from right to left\n",
    "        idxLR = string_to_integer_array(a[j],'1')\n",
    "        sourcesInNode.append(idxLR)  \n",
    "        sourcesNotInNode.append(list(set(sourceList) - set(idxLR)))\n",
    "        subset.append(node_subset(j,idxLR))\n",
    "\n",
    "    return sourcesInNode, subset\n",
    "\n",
    "\n",
    "def subset_to_indices(indices):\n",
    "    return [i for i in indices]\n",
    "\n",
    "class Choquet_integral(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, N_in, N_out):\n",
    "        super(Choquet_integral,self).__init__()\n",
    "        self.N_in = N_in\n",
    "        self.N_out = N_out\n",
    "        self.nVars = 2**self.N_in - 2\n",
    "        \n",
    "        # The FM is initialized with mean\n",
    "        dummy = (1./self.N_in) * torch.ones((self.nVars, self.N_out), requires_grad=True)\n",
    "#        self.vars = torch.nn.Parameter( torch.Tensor(self.nVars,N_out))\n",
    "        self.vars = torch.nn.Parameter(dummy)\n",
    "        \n",
    "        # following function uses numpy vs pytorch\n",
    "        self.sourcesInNode, self.subset = sources_and_subsets_nodes(self.N_in)\n",
    "        \n",
    "        self.sourcesInNode = [torch.tensor(x) for x in self.sourcesInNode]\n",
    "        self.subset = [torch.tensor(x) for x in self.subset]\n",
    "        \n",
    "    def forward(self,inputs):    \n",
    "        self.FM = self.chi_nn_vars(self.vars)\n",
    "        sortInputs, sortInd = torch.sort(inputs,1, True)\n",
    "        M, N = inputs.size()\n",
    "        sortInputs = torch.cat((sortInputs, torch.zeros(M,1)), 1)\n",
    "        sortInputs = sortInputs[:,:-1] -  sortInputs[:,1:]\n",
    "        \n",
    "        out = torch.cumsum(torch.pow(2,sortInd),1) - torch.ones(1, dtype=torch.int64)\n",
    "        \n",
    "        data = torch.zeros((M,self.nVars+1))\n",
    "        \n",
    "        for i in range(M):\n",
    "            data[i,out[i,:]] = sortInputs[i,:] \n",
    "        \n",
    "        \n",
    "        ChI = torch.matmul(data,self.FM)\n",
    "            \n",
    "        return ChI\n",
    "    \n",
    "    # Converts NN-vars to FM vars\n",
    "    def chi_nn_vars(self, chi_vars):\n",
    "#        nVars,_ = chi_vars.size()\n",
    "        chi_vars = torch.abs(chi_vars)\n",
    "        #        nInputs = inputs.get_shape().as_list()[1]\n",
    "        \n",
    "        FM = chi_vars[None, 0,:]\n",
    "        for i in range(1,self.nVars):\n",
    "            indices = subset_to_indices(self.subset[i])\n",
    "            if (len(indices) == 1):\n",
    "                FM = torch.cat((FM,chi_vars[None,i,:]),0)\n",
    "            else:\n",
    "                #         ss=tf.gather_nd(variables, [[1],[2]])\n",
    "                maxVal,_ = torch.max(FM[indices,:],0)\n",
    "                temp = torch.add(maxVal,chi_vars[i,:])\n",
    "                FM = torch.cat((FM,temp[None,:]),0)\n",
    "              \n",
    "        FM = torch.cat([FM, torch.ones((1,self.N_out))],0)\n",
    "        FM = torch.min(FM, torch.ones(1))  \n",
    "        \n",
    "        return FM\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac521a07",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# ChIQP\n",
    "\n",
    "import numpy as np\n",
    "import itertools\n",
    "from cvxopt import solvers, matrix\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Silencing solvers.qp\n",
    "\n",
    "from contextlib import redirect_stdout\n",
    "from io import StringIO\n",
    "class NullIO(StringIO):\n",
    "    def write(self, txt):\n",
    "        pass\n",
    "\n",
    "\n",
    "def silent(fn):\n",
    "    \"\"\"Decorator to silence functions.\"\"\"\n",
    "    def silent_fn(*args, **kwargs):\n",
    "        with redirect_stdout(NullIO()):\n",
    "            return fn(*args, **kwargs)\n",
    "    return silent_fn\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ChoquetIntegral:\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Instantiation of a ChoquetIntegral.\n",
    "           This sets up the ChI. It doesn't take any input parameters\n",
    "           because you may want to use pass your own values in(as opposed\n",
    "           to learning from data). To instatiate, use\n",
    "           chi = ChoquetIntegral.ChoquetIntegral()\n",
    "        \"\"\"\n",
    "        self.trainSamples, self.trainLabels = [], []\n",
    "        self.testSamples, self.testLabels = [], []\n",
    "        self.N, self.numberConstraints, self.M = 0, 0, 0\n",
    "        self.g = 0\n",
    "        self.fm = []\n",
    "        self.type = []\n",
    "\n",
    "\n",
    "    def train_chi(self, x1, l1):\n",
    "        \"\"\"\n",
    "        This trains this instance of your ChoquetIntegral w.r.t x1 and l1.\n",
    "        :param x1: These are the training samples of size N x M(inputs x number of samples)\n",
    "        :param l1: These are the training labels of size 1 x M(label per sample)\n",
    "        \"\"\"\n",
    "        self.type = 'quad'\n",
    "        self.trainSamples = x1\n",
    "        self.trainLabels = l1\n",
    "        self.N = self.trainSamples.shape[0]\n",
    "        self.M = self.trainSamples.shape[1]\n",
    "        print(\"Number Inputs : \", self.N, \"; Number Samples : \", self.M)\n",
    "        self.fm = self.produce_lattice()\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "\n",
    "    def chi_quad(self, x2):\n",
    "        \"\"\"\n",
    "        This will produce an output for this instance of the ChI\n",
    "        This will use the learned(or specified) Choquet integral to\n",
    "        produce an output w.r.t. to the new input.\n",
    "        :param x2: testing sample\n",
    "        :return: output of the choquet integral.\n",
    "        \"\"\"\n",
    "        if self.type == 'quad':\n",
    "            n = len(x2)\n",
    "            pi_i = np.argsort(x2)[::-1][:n] + 1\n",
    "            ch = x2[pi_i[0] - 1] * (self.fm[str(pi_i[:1])])\n",
    "            for i in range(1, n):\n",
    "                latt_pti = np.sort(pi_i[:i + 1])\n",
    "                latt_ptimin1 = np.sort(pi_i[:i])\n",
    "                ch = ch + x2[pi_i[i] - 1] * (self.fm[str(latt_pti)] - self.fm[str(latt_ptimin1)])\n",
    "            return ch\n",
    "        else:\n",
    "            print(\"If using sugeno measure, you need to use chi_sugeno.\")\n",
    "\n",
    "\n",
    "    def produce_lattice(self):\n",
    "        \"\"\"\n",
    "            This method builds is where the lattice(or FM variables) will be learned.\n",
    "            The FM values can be found via a quadratic program, which is used here\n",
    "            after setting up constraint matrices. Refer to papers for complete overview.\n",
    "        :return: Lattice, the learned FM variables.\n",
    "        \"\"\"\n",
    "\n",
    "        fm_len = 2 ** self.N - 1  # nc\n",
    "        E = np.zeros((fm_len, fm_len))  # D\n",
    "        L = np.zeros(fm_len)  # f\n",
    "        index_keys = self.get_keys_index()\n",
    "        for i in range(0, self.M):  # it's going through one sample at a time.\n",
    "            l = self.trainLabels[i]  # this is the labels\n",
    "            fm_coeff = self.get_fm_class_img_coeff(index_keys, self.trainSamples[:, i], fm_len)  # this is Hdiff\n",
    "            # print(fm_coeff)\n",
    "            L = L + (-2) * l * fm_coeff\n",
    "            E = E + np.matmul(fm_coeff.reshape((fm_len, 1)), fm_coeff.reshape((1, fm_len)))\n",
    "\n",
    "        G, h, A, b = self.build_constraint_matrices(index_keys, fm_len)\n",
    "        solvers_qp = silent(solvers.qp)\n",
    "        sol = solvers_qp(matrix(2 * E, tc='d'), matrix(L.T, tc='d'), matrix(G, tc='d'), matrix(h, tc='d'),\n",
    "                         matrix(A, tc='d'), matrix(b, tc='d'))\n",
    "        g = sol['x']\n",
    "        Lattice = {}\n",
    "        for key in index_keys.keys():\n",
    "            Lattice[key] = g[index_keys[key]]\n",
    "        return Lattice\n",
    "\n",
    "\n",
    "    def build_constraint_matrices(self, index_keys, fm_len):\n",
    "        \"\"\"\n",
    "        This method builds the necessary constraint matrices.\n",
    "        :param index_keys: map to reference lattice components\n",
    "        :param fm_len: length of the fuzzy measure\n",
    "        :return: the constraint matrices\n",
    "        \"\"\"\n",
    "\n",
    "        vls = np.arange(1, self.N + 1)\n",
    "        line = np.zeros(fm_len)\n",
    "        G = line\n",
    "        line[index_keys[str(np.array([1]))]] = -1.\n",
    "        h = np.array([0])\n",
    "        for i in range(2, self.N + 1):\n",
    "            line = np.zeros(fm_len)\n",
    "            line[index_keys[str(np.array([i]))]] = -1.\n",
    "            G = np.vstack((G, line))\n",
    "            h = np.vstack((h, np.array([0])))\n",
    "        for i in range(2, self.N + 1):\n",
    "            parent = np.array(list(itertools.combinations(vls, i)))\n",
    "            for latt_pt in parent:\n",
    "                for j in range(len(latt_pt) - 1, len(latt_pt)):\n",
    "                    children = np.array(list(itertools.combinations(latt_pt, j)))\n",
    "                    for latt_ch in children:\n",
    "                        line = np.zeros(fm_len)\n",
    "                        line[index_keys[str(latt_ch)]] = 1.\n",
    "                        line[index_keys[str(latt_pt)]] = -1.\n",
    "                        G = np.vstack((G, line))\n",
    "                        h = np.vstack((h, np.array([0])))\n",
    "\n",
    "        line = np.zeros(fm_len)\n",
    "        line[index_keys[str(vls)]] = 1.\n",
    "        G = np.vstack((G, line))\n",
    "        h = np.vstack((h, np.array([1])))\n",
    "\n",
    "        # equality constraints\n",
    "        A = np.zeros((1, fm_len))\n",
    "        A[0, -1] = 1\n",
    "        b = np.array([1]);\n",
    "\n",
    "        return G, h, A, b\n",
    "\n",
    "\n",
    "    def get_fm_class_img_coeff(self, Lattice, h, fm_len):  # Lattice is FM_name_and_index, h is the samples, fm_len\n",
    "        \"\"\"\n",
    "        This creates a FM map with the name as the key and the index as the value\n",
    "        :param Lattice: dictionary with FM\n",
    "        :param h: sample\n",
    "        :param fm_len: fm length\n",
    "        :return: the fm_coeff\n",
    "        \"\"\"\n",
    "\n",
    "        n = len(h)  # len(h) is the number of the samples\n",
    "        fm_coeff = np.zeros(fm_len)\n",
    "        pi_i = np.argsort(h)[::-1][:n] + 1\n",
    "        for i in range(1, n):\n",
    "            fm_coeff[Lattice[str(np.sort(pi_i[:i]))]] = h[pi_i[i - 1] - 1] - h[pi_i[i] - 1]\n",
    "        fm_coeff[Lattice[str(np.sort(pi_i[:n]))]] = h[pi_i[n - 1] - 1]\n",
    "        np.matmul(fm_coeff, np.transpose(fm_coeff))\n",
    "        return fm_coeff\n",
    "\n",
    "\n",
    "    def get_keys_index(self):\n",
    "        \"\"\"\n",
    "        Sets up a dictionary for referencing FM.\n",
    "        :return: The keys to the dictionary\n",
    "        \"\"\"\n",
    "\n",
    "        vls = np.arange(1, self.N + 1)\n",
    "        count = 0\n",
    "        Lattice = {}\n",
    "        for i in range(0, self.N):\n",
    "            Lattice[str(np.array([vls[i]]))] = count\n",
    "            count = count + 1\n",
    "        for i in range(2, self.N + 1):\n",
    "            A = np.array(list(itertools.combinations(vls, i)))\n",
    "            for latt_pt in A:\n",
    "                Lattice[str(latt_pt)] = count\n",
    "                count = count + 1\n",
    "        return Lattice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dab322c4",
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1, 2), (2, 0, 1), (1, 0, 2), (2, 1, 0), (0, 2, 1), (1, 2, 0)]\n",
      "Train data size\n",
      "(3, 200)\n",
      "All permutations\n",
      "0 (0, 1, 2)\n",
      "1 (2, 0, 1)\n",
      "2 (1, 0, 2)\n",
      "3 (2, 1, 0)\n",
      "4 (0, 2, 1)\n",
      "5 (1, 2, 0)\n"
     ]
    }
   ],
   "source": [
    "# Creating data\n",
    "\n",
    "num_train = 200\n",
    "dim = 3\n",
    "num_test = 1000\n",
    "\n",
    "# Get the N! possible permutations\n",
    "all_perms = list(permutations(list(range(dim))))\n",
    "random.shuffle(all_perms)\n",
    "print(all_perms)\n",
    "\n",
    "# Get the sort of each data sample\n",
    "train_data = np.random.rand(dim, num_train)\n",
    "train_data_perms = np.argsort(train_data, 0)\n",
    "print('Train data size')\n",
    "print(train_data.shape)\n",
    "\n",
    "# Divide data into N! sets according to its permutation\n",
    "train_data_div_by_perm = []\n",
    "\n",
    "print('All permutations')\n",
    "for i, current_perm in enumerate(all_perms):\n",
    "    print(i, current_perm)\n",
    "    temp = np.where(train_data_perms[0, :]==current_perm[0])\n",
    "    for j, idx in enumerate(current_perm):\n",
    "#         print(j, idx)\n",
    "        temp = np.intersect1d(temp, np.where(train_data_perms[j, :]==idx))\n",
    "    train_data_div_by_perm.append(temp)\n",
    "# print(train_data_div_by_perm)\n",
    "# print(train_data_permutation[:, train_data_div_by_perm[0]])\n",
    "\n",
    "\n",
    "\n",
    "# Test data\n",
    "test_data = np.random.rand(dim, num_test)\n",
    "test_data_perms = np.argsort(test_data, 0)\n",
    "\n",
    "# Divide data into N! sets according to its permutation\n",
    "test_data_div_by_perm = []\n",
    "\n",
    "for i, current_perm in enumerate(all_perms):\n",
    "    temp = np.where(test_data_perms[0, :]==current_perm[0])\n",
    "    for j, idx in enumerate(current_perm):\n",
    "        temp = np.intersect1d(temp, np.where(test_data_perms[j, :]==idx))\n",
    "    test_data_div_by_perm.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83055073",
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data size\n",
      "(3, 41)\n",
      "(3, 66)\n",
      "(3, 92)\n",
      "(3, 127)\n",
      "(3, 179)\n",
      "(3, 200)\n",
      "test data size\n",
      "(3, 165)\n",
      "(3, 350)\n",
      "(3, 514)\n",
      "(3, 678)\n",
      "(3, 828)\n",
      "(3, 1000)\n",
      "Test data complement size\n",
      "(3, 835)\n",
      "(3, 650)\n",
      "(3, 486)\n",
      "(3, 322)\n",
      "(3, 172)\n"
     ]
    }
   ],
   "source": [
    "# Making N! train datasets with data percentage = i/N!\n",
    "train_data_ioverN_percent = [train_data_div_by_perm[0]]\n",
    "for i in range(1, len(all_perms)):\n",
    "    train_data_iN = np.concatenate((train_data_ioverN_percent[i-1], train_data_div_by_perm[i]))\n",
    "    random.shuffle(train_data_iN)\n",
    "    train_data_ioverN_percent.append(train_data_iN)\n",
    "    \n",
    "print('train data size')\n",
    "for i, data_idx in enumerate(train_data_ioverN_percent):\n",
    "    train_d = train_data[:, data_idx]\n",
    "    print(train_d.shape)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "# Making N! test datasets with data percentage = i/N!\n",
    "test_data_ioverN_percent = [test_data_div_by_perm[0]]\n",
    "test_data_ioverN_percent_c = []\n",
    "for i in range(1, len(all_perms)):\n",
    "    test_data_iN = np.concatenate((test_data_ioverN_percent[i-1], test_data_div_by_perm[i]))\n",
    "    test_data_iN_c = list(set(range(num_test)) - set(test_data_ioverN_percent[i-1]))\n",
    "    test_data_ioverN_percent.append(test_data_iN)\n",
    "    test_data_ioverN_percent_c.append(test_data_iN_c)\n",
    "    \n",
    "print('test data size')\n",
    "for i, data_idx in enumerate(test_data_ioverN_percent):\n",
    "    test_d = test_data[:, data_idx]\n",
    "    print(test_d.shape)\n",
    "\n",
    "print('Test data complement size')\n",
    "for i, data_idx in enumerate(test_data_ioverN_percent_c):\n",
    "    test_d_c = test_data[:, data_idx]\n",
    "    print(test_d_c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "359f9c47",
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seen data percentage: 0.16666666666666666\n",
      "Number Inputs :  3 ; Number Samples :  41\n",
      "\n",
      "Chi min {'[1]': 0.2354879142068756, '[2]': 0.0001849275946498831, '[3]': 1.8637436170893987e-05, '[1 2]': 0.719153104800557, '[1 3]': 0.7122960354239325, '[2 3]': 0.0003870308197371083, '[1 2 3]': 1.0}\n",
      "\n",
      "SSE 2.3781459239824693e-06\n",
      "\n",
      "Unseen data SSE 42.94300935129752\n",
      "Number Inputs :  3 ; Number Samples :  41\n",
      "\n",
      "Chi max {'[1]': 0.2794067654824745, '[2]': 0.287202606481952, '[3]': 0.9990570883064791, '[1 2]': 0.7646721188148462, '[1 3]': 0.9995489603566573, '[2 3]': 0.9999536491448996, '[1 2 3]': 1.0}\n",
      "\n",
      "SSE 1.369233329855145e-05\n",
      "\n",
      "Unseen data SSE 40.23260729702878\n",
      "Number Inputs :  3 ; Number Samples :  41\n",
      "\n",
      "Chi mean {'[1]': 0.23381603250379973, '[2]': 0.2150156433321878, '[3]': 0.33333330146158896, '[1 2]': 0.7656312561380358, '[1 3]': 0.7848534138080756, '[2 3]': 0.6666667011697853, '[1 2 3]': 1.0}\n",
      "\n",
      "SSE 1.6662938156904184e-14\n",
      "\n",
      "Unseen data SSE 1.0290310829847267\n",
      "Number Inputs :  3 ; Number Samples :  41\n",
      "\n",
      "Chi geometric mean {'[1]': 0.23308959634441737, '[2]': 0.17000722744611665, '[3]': 0.20878916084575386, '[1 2]': 0.7563952999651212, '[1 3]': 0.7559723537308384, '[2 3]': 0.4760904078754469, '[1 2 3]': 1.0}\n",
      "\n",
      "SSE 0.37079942527983334\n",
      "\n",
      "Unseen data SSE 8.309380598351485\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Seen data percentage: 0.3333333333333333\n",
      "Number Inputs :  3 ; Number Samples :  66\n",
      "\n",
      "Chi min {'[1]': 0.000266989763926378, '[2]': 2.0875445157837207e-05, '[3]': 5.33014795862295e-05, '[1 2]': 0.000546295586858269, '[1 3]': 0.6562121917546229, '[2 3]': 0.00038113479164747196, '[1 2 3]': 1.0}\n",
      "\n",
      "SSE 7.959272462673703e-06\n",
      "\n",
      "Unseen data SSE 15.708913395802362\n",
      "Number Inputs :  3 ; Number Samples :  66\n",
      "\n",
      "Chi max {'[1]': 0.34044957323310493, '[2]': 0.9988168299240706, '[3]': 0.9989581117696709, '[1 2]': 0.9998311131469894, '[1 3]': 0.9994694846035521, '[2 3]': 0.9999523420666059, '[1 2 3]': 1.0}\n",
      "\n",
      "SSE 5.1478374879390806e-05\n",
      "\n",
      "Unseen data SSE 12.461774218800421\n",
      "Number Inputs :  3 ; Number Samples :  66\n",
      "\n",
      "Chi mean {'[1]': 0.2127140868231955, '[2]': 0.3333331374057102, '[3]': 0.33333328063406553, '[1 2]': 0.6666668490074796, '[1 3]': 0.7847186000445152, '[2 3]': 0.6666667380239661, '[1 2 3]': 1.0}\n",
      "\n",
      "SSE 7.546406091640557e-13\n",
      "\n",
      "Unseen data SSE 0.6864135599529899\n",
      "Number Inputs :  3 ; Number Samples :  66\n",
      "\n",
      "Chi geometric mean {'[1]': 0.15398596585283567, '[2]': 0.19451962200780357, '[3]': 0.20878911702977357, '[1 2]': 0.4330130364944785, '[1 3]': 0.7451394105217778, '[2 3]': 0.4760905000346196, '[1 2 3]': 1.0}\n",
      "\n",
      "SSE 0.7314237248384188\n",
      "\n",
      "Unseen data SSE 4.326417747755532\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Seen data percentage: 0.5\n",
      "Number Inputs :  3 ; Number Samples :  92\n",
      "\n",
      "Chi min {'[1]': 0.00015065263624367922, '[2]': 3.64064687680536e-05, '[3]': 2.1281813758198926e-05, '[1 2]': 0.0005202643487030802, '[1 3]': 0.0004941129270964851, '[2 3]': 0.00040156698862765295, '[1 2 3]': 1.0}\n",
      "\n",
      "SSE 1.2622742700258872e-05\n",
      "\n",
      "Unseen data SSE 1.438159348670894e-05\n",
      "Number Inputs :  3 ; Number Samples :  92\n",
      "\n",
      "Chi max {'[1]': 0.34935270387193873, '[2]': 0.9988496455104138, '[3]': 0.9992722336196735, '[1 2]': 0.9998551547673108, '[1 3]': 0.9998307070679359, '[2 3]': 0.9999554412443907, '[1 2 3]': 1.0}\n",
      "\n",
      "SSE 5.195597522446173e-05\n",
      "\n",
      "Unseen data SSE 12.123500908051788\n",
      "Number Inputs :  3 ; Number Samples :  92\n",
      "\n",
      "Chi mean {'[1]': 0.20170815658809252, '[2]': 0.33333313070956844, '[3]': 0.33333328082236313, '[1 2]': 0.6666668549667065, '[1 3]': 0.6666667154671193, '[2 3]': 0.6666667389812639, '[1 2 3]': 1.0}\n",
      "\n",
      "SSE 8.498253246266779e-13\n",
      "\n",
      "Unseen data SSE 0.4960173350562945\n",
      "Number Inputs :  3 ; Number Samples :  92\n",
      "\n",
      "Chi geometric mean {'[1]': 0.133438840033386, '[2]': 0.19451951955101984, '[3]': 0.18885962134574213, '[1 2]': 0.4330131426688691, '[1 3]': 0.49472930258886905, '[2 3]': 0.48639208636248593, '[1 2 3]': 1.0}\n",
      "\n",
      "SSE 1.1086893187865272\n",
      "\n",
      "Unseen data SSE 1.2271648879136499\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Seen data percentage: 0.6666666666666666\n",
      "Number Inputs :  3 ; Number Samples :  127\n",
      "\n",
      "Chi min {'[1]': 4.430524943676477e-05, '[2]': 3.971129480478673e-05, '[3]': 1.98782558654678e-05, '[1 2]': 0.00032434195329939777, '[1 3]': 0.0004657226159897719, '[2 3]': 0.0003964928988822978, '[1 2 3]': 1.0}\n",
      "\n",
      "SSE 1.0983797273248766e-05\n",
      "\n",
      "Unseen data SSE 6.720233219690145e-06\n",
      "Number Inputs :  3 ; Number Samples :  127\n",
      "\n",
      "Chi max {'[1]': 0.9988384943710739, '[2]': 0.9987147397838333, '[3]': 0.9992020276503537, '[1 2]': 0.9999507196956924, '[1 3]': 0.9998685392481007, '[2 3]': 0.9998921485773572, '[1 2 3]': 1.0}\n",
      "\n",
      "SSE 8.004727491430608e-05\n",
      "\n",
      "Unseen data SSE 4.6332518858937846e-05\n",
      "Number Inputs :  3 ; Number Samples :  127\n",
      "\n",
      "Chi mean {'[1]': 0.33333323374075624, '[2]': 0.3333332002956864, '[3]': 0.3333332750785294, '[1 2]': 0.6666667678558679, '[1 3]': 0.6666667441822093, '[2 3]': 0.6666667382700697, '[1 2 3]': 1.0}\n",
      "\n",
      "SSE 6.054456111986823e-13\n",
      "\n",
      "Unseen data SSE 2.8403400243353344e-13\n",
      "Number Inputs :  3 ; Number Samples :  127\n",
      "\n",
      "Chi geometric mean {'[1]': 0.22803222702890655, '[2]': 0.1684988250328982, '[3]': 0.18885962477754756, '[1 2]': 0.4763678701766166, '[1 3]': 0.4947293681041955, '[2 3]': 0.486392037939554, '[1 2 3]': 1.0}\n",
      "\n",
      "SSE 1.5027651963436894\n",
      "\n",
      "Unseen data SSE 0.7525650006280429\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Seen data percentage: 0.8333333333333334\n",
      "Number Inputs :  3 ; Number Samples :  179\n",
      "\n",
      "Chi min {'[1]': 4.132075736537399e-05, '[2]': 1.7486663773497984e-05, '[3]': 1.998890678189051e-05, '[1 2]': 0.00032271472626781226, '[1 3]': 0.0004607385185363017, '[2 3]': 0.0002537879744398814, '[1 2 3]': 1.0}\n",
      "\n",
      "SSE 1.0273836882175606e-05\n",
      "\n",
      "Unseen data SSE 3.95549548990867e-06\n",
      "Number Inputs :  3 ; Number Samples :  179\n",
      "\n",
      "Chi max {'[1]': 0.9988747184327338, '[2]': 0.9993169945505449, '[3]': 0.9992256485844817, '[1 2]': 0.999953782536212, '[1 3]': 0.9998809102464942, '[2 3]': 0.9999541710791453, '[1 2 3]': 1.0}\n",
      "\n",
      "SSE 5.6710841761143235e-05\n",
      "\n",
      "Unseen data SSE 2.1128955090488185e-05\n",
      "Number Inputs :  3 ; Number Samples :  179\n",
      "\n",
      "Chi mean {'[1]': 0.3333332671399925, '[2]': 0.33333330372012854, '[3]': 0.33333329765955744, '[1 2]': 0.6666667157320402, '[1 3]': 0.6666667273745265, '[2 3]': 0.6666666935622013, '[1 2 3]': 1.0}\n",
      "\n",
      "SSE 1.6399488929354823e-13\n",
      "\n",
      "Unseen data SSE 5.993486133685899e-14\n",
      "Number Inputs :  3 ; Number Samples :  179\n",
      "\n",
      "Chi geometric mean {'[1]': 0.2309623954784218, '[2]': 0.187132772037129, '[3]': 0.19277062525714925, '[1 2]': 0.4700525906448007, '[1 3]': 0.4931624472153376, '[2 3]': 0.4748397387436246, '[1 2 3]': 1.0}\n",
      "\n",
      "SSE 1.8082071599437102\n",
      "\n",
      "Unseen data SSE 0.4256997041015419\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Seen data percentage: 1.0\n",
      "Number Inputs :  3 ; Number Samples :  200\n",
      "\n",
      "Chi min {'[1]': 1.990003822949146e-05, '[2]': 1.6349701901651204e-05, '[3]': 1.8696235404307526e-05, '[1 2]': 0.0003124830825012062, '[1 3]': 0.00031427962571158947, '[2 3]': 0.00024538622554856496, '[1 2 3]': 1.0}\n",
      "\n",
      "SSE 9.298341069918612e-06\n",
      "Number Inputs :  3 ; Number Samples :  200\n",
      "\n",
      "Chi max {'[1]': 0.999108151771555, '[2]': 0.9993342046203284, '[3]': 0.9992468136320514, '[1 2]': 0.9999560767199234, '[1 3]': 0.9999327377124461, '[2 3]': 0.999959160521231, '[1 2 3]': 1.0}\n",
      "\n",
      "SSE 5.984877360428057e-05\n",
      "Number Inputs :  3 ; Number Samples :  200\n",
      "\n",
      "Chi mean {'[1]': 0.33333329005012796, '[2]': 0.33333330798070626, '[3]': 0.3333333074498652, '[1 2]': 0.6666667054290477, '[1 3]': 0.6666666983109157, '[2 3]': 0.6666666891899301, '[1 2 3]': 1.0}\n",
      "\n",
      "SSE 1.0010016306168911e-13\n",
      "Number Inputs :  3 ; Number Samples :  200\n",
      "\n",
      "Chi geometric mean {'[1]': 0.23645302114201086, '[2]': 0.18835330875186862, '[3]': 0.2030646089907701, '[1 2]': 0.4681597272091869, '[1 3]': 0.44141927506298384, '[2 3]': 0.4720367715863519, '[1 2 3]': 1.0}\n",
      "\n",
      "SSE 2.1901190501962735\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "\n",
    "eva_func_num = 4\n",
    "\n",
    "SSEs = np.zeros((len(all_perms), eva_func_num))\n",
    "SSEs_c = np.zeros((len(all_perms)-1, eva_func_num))\n",
    "\n",
    "for i, data_idx in enumerate(train_data_ioverN_percent):\n",
    "    print('Seen data percentage:', (i+1)/math.factorial(dim))\n",
    "    \n",
    "    train_d = train_data[:, data_idx]\n",
    "    test_d = test_data[:, test_data_ioverN_percent[i]]\n",
    "    \n",
    "    train_label_min = np.amin(train_d, 0)\n",
    "    train_label_max = np.amax(train_d, 0)\n",
    "    train_label_mean = np.mean(train_d, 0)\n",
    "    train_label_gmean = np.cbrt(np.prod(train_d, 0))\n",
    "    \n",
    "    test_label_min = np.amin(test_d, 0)\n",
    "    test_label_max = np.amax(test_d, 0)\n",
    "    test_label_mean = np.mean(test_d, 0)\n",
    "    test_label_gmean = np.cbrt(np.prod(test_d, 0))\n",
    "    \n",
    "    if i < len(all_perms)-1:\n",
    "        test_d_c = test_data[:, test_data_ioverN_percent_c[i]]\n",
    "\n",
    "        test_label_min_c = np.amin(test_d_c, 0)\n",
    "        test_label_max_c = np.amax(test_d_c, 0)\n",
    "        test_label_mean_c = np.mean(test_d_c, 0)\n",
    "        test_label_gmean_c = np.cbrt(np.prod(test_d_c, 0))\n",
    "    \n",
    "    \n",
    "    chi = ChoquetIntegral()\n",
    "    chi.train_chi(train_d, train_label_min)\n",
    "    SSE = 0\n",
    "    for j in range(np.size(test_d, 1)):\n",
    "        chi_min_test = chi.chi_quad(test_d[:, j])\n",
    "        SSE += (chi_min_test - test_label_min[j]) ** 2\n",
    "    SSEs[i, 0] = SSE / np.size(test_d)\n",
    "    print('\\nChi min', chi.fm)\n",
    "    print('\\nSSE', SSE)\n",
    "    if i < len(all_perms)-1:\n",
    "        SSE_c = 0\n",
    "        for j in range(np.size(test_d_c, 1)):\n",
    "            chi_min_test_c = chi.chi_quad(test_d_c[:, j])\n",
    "            SSE_c += (chi_min_test_c - test_label_min_c[j]) ** 2\n",
    "        SSEs_c[i, 0] = SSE_c / np.size(test_d_c)\n",
    "        print('\\nUnseen data SSE', SSE_c)\n",
    "        \n",
    "\n",
    "    chi = ChoquetIntegral()\n",
    "    chi.train_chi(train_d, train_label_max)\n",
    "    SSE = 0\n",
    "    for j in range(np.size(test_d, 1)):\n",
    "        chi_max_test = chi.chi_quad(test_d[:, j])\n",
    "        SSE += (chi_max_test - test_label_max[j]) ** 2\n",
    "    SSEs[i, 1] = SSE / np.size(test_d)\n",
    "    print('\\nChi max', chi.fm)\n",
    "    print('\\nSSE', SSE)\n",
    "    if i < len(all_perms)-1:\n",
    "        SSE_c = 0\n",
    "        for j in range(np.size(test_d_c, 1)):\n",
    "            chi_max_test_c = chi.chi_quad(test_d_c[:, j])\n",
    "            SSE_c += (chi_max_test_c - test_label_max_c[j]) ** 2\n",
    "        SSEs_c[i, 1] = SSE_c / np.size(test_d_c)\n",
    "        print('\\nUnseen data SSE', SSE_c)\n",
    "        \n",
    "        \n",
    "    chi = ChoquetIntegral()\n",
    "    chi.train_chi(train_d, train_label_mean)\n",
    "    SSE = 0\n",
    "    for j in range(np.size(test_d, 1)):\n",
    "        chi_mean_test = chi.chi_quad(test_d[:, j])\n",
    "        SSE += (chi_mean_test - test_label_mean[j]) ** 2\n",
    "    SSEs[i, 2] = SSE / np.size(test_d)\n",
    "    print('\\nChi mean', chi.fm)\n",
    "    print('\\nSSE', SSE)\n",
    "    if i < len(all_perms)-1:\n",
    "        SSE_c = 0\n",
    "        for j in range(np.size(test_d_c, 1)):\n",
    "            chi_mean_test_c = chi.chi_quad(test_d_c[:, j])\n",
    "            SSE_c += (chi_mean_test_c - test_label_mean_c[j]) ** 2\n",
    "        SSEs_c[i, 2] = SSE_c / np.size(test_d_c)\n",
    "        print('\\nUnseen data SSE', SSE_c)\n",
    "        \n",
    "        \n",
    "    chi = ChoquetIntegral()\n",
    "    chi.train_chi(train_d, train_label_gmean)\n",
    "    SSE = 0\n",
    "    for j in range(np.size(test_d, 1)):\n",
    "        chi_gmean_test = chi.chi_quad(test_d[:, j])\n",
    "        SSE += (chi_gmean_test - test_label_gmean[j]) ** 2\n",
    "    SSEs[i, 3] = SSE / np.size(test_d)\n",
    "    print('\\nChi geometric mean', chi.fm)\n",
    "    print('\\nSSE', SSE)\n",
    "    if i < len(all_perms)-1:\n",
    "        SSE_c = 0\n",
    "        for j in range(np.size(test_d_c, 1)):\n",
    "            chi_gmean_test_c = chi.chi_quad(test_d_c[:, j])\n",
    "            SSE_c += (chi_gmean_test_c - test_label_gmean_c[j]) ** 2\n",
    "        SSEs_c[i, 3] = SSE_c / np.size(test_d_c)\n",
    "        print('\\nUnseen data SSE', SSE_c)\n",
    "    \n",
    "    \n",
    "    print('\\n\\n\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e274457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6,) (6, 4)\n",
      "[0.16666667 0.33333333 0.5        0.66666667 0.83333333 1.        ]\n",
      "[[4.80433520e-09 2.76612794e-08 3.36625013e-17 7.49089748e-04]\n",
      " [7.58025949e-09 4.90270237e-08 7.18705342e-16 6.96594024e-04]\n",
      " [8.18595506e-09 3.36938880e-08 5.51118888e-16 7.18994370e-04]\n",
      " [5.40009699e-09 3.93546091e-08 2.97662542e-16 7.38822614e-04]\n",
      " [4.13600519e-09 2.28304516e-08 6.60204868e-17 7.27941691e-04]\n",
      " [3.09944702e-09 1.99495912e-08 3.33667210e-17 7.30039683e-04]]\n"
     ]
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "x = (np.asarray(list(range(1, len(all_perms)+1)))) / len(all_perms)\n",
    "print(x.shape, SSEs.shape)\n",
    "print(x)\n",
    "print(SSEs)\n",
    "plt.plot(x, SSEs)\n",
    "ax.set_title('SSE (Data with same pattern)')\n",
    "ax.legend(['Min', 'Max', 'Mean', 'Geometric Mean'])\n",
    "ax.set_xlabel('Percentage of Seen Data')\n",
    "ax.set_ylabel('SSE')\n",
    "ax.xaxis.set_major_formatter(FuncFormatter('{0:.0%}'.format))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d21df8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,) (5, 4)\n",
      "[0.16666667 0.33333333 0.5        0.66666667 0.83333333]\n",
      "[[1.71429179e-02 1.60609211e-02 4.10790851e-04 3.31711800e-03]\n",
      " [8.05585302e-03 6.39065345e-03 3.52006954e-04 2.21867577e-03]\n",
      " [9.86391872e-09 8.31515837e-03 3.40203934e-04 8.41676878e-04]\n",
      " [6.95676317e-09 4.79632700e-08 2.94031058e-16 7.79052796e-04]\n",
      " [7.66568893e-09 4.09475874e-08 1.16152832e-16 8.24999427e-04]]\n"
     ]
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "x = (np.asarray(list(range(1, len(all_perms))))) / len(all_perms)\n",
    "print(x.shape, SSEs_c.shape)\n",
    "print(x)\n",
    "print(SSEs_c)\n",
    "plt.plot(x, SSEs_c)\n",
    "ax.set_title('SSE (Data with unseen pattern)')\n",
    "ax.legend(['Min', 'Max', 'Mean', 'Geometric Mean'])\n",
    "ax.set_xlabel('Percentage of Seen Data')\n",
    "ax.set_ylabel('SSE')\n",
    "ax.xaxis.set_major_formatter(FuncFormatter('{0:.0%}'.format))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b443c791",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-c433f6a4d9b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;31m# Compute the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;31m# Zero gradients, perform a backward pass, and update the weights.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/nn/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/nn/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/nn/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2201\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreduction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'mean'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2202\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2203\u001b[0;31m         \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2204\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2205\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/nn/lib/python3.7/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 [0, 1, 2]])\n\u001b[1;32m     51\u001b[0m     \"\"\"\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# NN test\n",
    "    \n",
    "# training samples size\n",
    "M = 700\n",
    "\n",
    "# number of inputs\n",
    "N_in = 3\n",
    "\n",
    "# number of outputs aka number of Choquet integral neurons\n",
    "N_out = 2  \n",
    "\n",
    "# Create a synthetic dataset via random sampling from a normal distribution with mean =-1 and std=2\n",
    "X_train = np.random.rand(M,N_in)*2-1\n",
    "\n",
    "# Let's specify the FMs  (There will be N_out number of FMs)\n",
    "# Herein we adopt binary encoding instead of lexicographic encoding to represent a FM that is easier to code. \n",
    "# As for example, an FM for three inputs using lexicographic encoding is, g = {g_1, g_2, g_3, g_{12}, g_{13}, g_{23}, g_{123}}.\n",
    "# whereas its binary encoding is g = {g_1, g_2, g_{12}, g_3 g_{13}, g_{23}, g_{123}}.\n",
    "\n",
    "# For simplicity, here we use OWA. \n",
    "\n",
    "# OWA = np.array([[0.7, 0.2, 0.1], # this is soft-max\n",
    "#                 [0.1,0.2,0.7]])  # soft-min\n",
    "\n",
    "# The FMs of the above OWAs in binary encoding\n",
    "# FM = [[0.7, 0.7, 0.9, 0.7, 0.9, 0.9, 1.0].\n",
    "#      [0.1, 0.1, 0.3, 0.1, 0.3, 0.3, 1.0]]\n",
    "\n",
    "# print('Actual/groundtruth FMs in binary encoding:')\n",
    "# print('FM1 = ', np.array([0.7, 0.7, 0.9, 0.7, 0.9, 0.9, 1.0]))\n",
    "# print('FM2 = ', np.array([0.1, 0.1, 0.3, 0.1, 0.3, 0.3, 1.0]))\n",
    "\n",
    "# Generate the label or the groundtruth based on the provided FMs/OWAs. The labels are two dimentional\n",
    "# label_train = np.matmul(np.sort(X_train), np.fliplr(OWA).T)\n",
    "label_train = np.amax(X_train, 0)\n",
    "\n",
    "# Now we want to recover the FMs from the training data and groundtruth\n",
    "# First, build a Choquet integral neuron with N_in inputs and N_out outputs\n",
    "net = Choquet_integral(N_in,N_out)\n",
    "\n",
    "# set the optimization algorithms and paramters the learning\n",
    "learning_rate = 0.3;\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "criterion = torch.nn.MSELoss(reduction='mean')\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate)   \n",
    "\n",
    "num_epochs = 300;\n",
    "\n",
    "# convert from numpy to torch tensor\n",
    "X_train = torch.tensor(X_train,dtype=torch.float)\n",
    "label_train = torch.tensor(label_train,dtype=torch.float)\n",
    "\n",
    "# optimize\n",
    "for t in range(num_epochs):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = net(X_train)\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = criterion(y_pred, label_train)\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()  \n",
    "\n",
    "# Finally, the learned FMs\n",
    "FM_learned = (net.chi_nn_vars(net.vars).cpu()).detach().numpy()\n",
    "print('\\n\\nLearned FMs:')\n",
    "print('FM1 = ', FM_learned[:,0])\n",
    "print('FM2 = ',FM_learned[:,1])\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}