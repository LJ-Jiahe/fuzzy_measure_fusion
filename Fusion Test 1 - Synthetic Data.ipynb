{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820570bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "715b328e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, models,datasets\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54248cff",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# ChINN\n",
    "\n",
    "# Convert decimal to binary string\n",
    "def sources_and_subsets_nodes(N):\n",
    "    str1 = \"{0:{fill}\"+str(N)+\"b}\"\n",
    "    a = []\n",
    "    for i in range(1,2**N):\n",
    "        a.append(str1.format(i, fill='0'))\n",
    "\n",
    "    sourcesInNode = []\n",
    "    sourcesNotInNode = []\n",
    "    subset = []\n",
    "    sourceList = list(range(N))\n",
    "    # find subset nodes of a node\n",
    "    def node_subset(node, sourcesInNodes):\n",
    "        return [node - 2**(i) for i in sourcesInNodes]\n",
    "    \n",
    "    # convert binary encoded string to integer list\n",
    "    def string_to_integer_array(s, ch):\n",
    "        N = len(s) \n",
    "        return [(N - i - 1) for i, ltr in enumerate(s) if ltr == ch]\n",
    "    \n",
    "    for j in range(len(a)):\n",
    "        # index from right to left\n",
    "        idxLR = string_to_integer_array(a[j],'1')\n",
    "        sourcesInNode.append(idxLR)  \n",
    "        sourcesNotInNode.append(list(set(sourceList) - set(idxLR)))\n",
    "        subset.append(node_subset(j,idxLR))\n",
    "\n",
    "    return sourcesInNode, subset\n",
    "\n",
    "\n",
    "def subset_to_indices(indices):\n",
    "    return [i for i in indices]\n",
    "\n",
    "class Choquet_integral(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, N_in, N_out):\n",
    "        super(Choquet_integral,self).__init__()\n",
    "        self.N_in = N_in\n",
    "        self.N_out = N_out\n",
    "        self.nVars = 2**self.N_in - 2\n",
    "        \n",
    "        # The FM is initialized with mean\n",
    "        dummy = (1./self.N_in) * torch.ones((self.nVars, self.N_out), requires_grad=True)\n",
    "#        self.vars = torch.nn.Parameter( torch.Tensor(self.nVars,N_out))\n",
    "        self.vars = torch.nn.Parameter(dummy)\n",
    "        \n",
    "        # following function uses numpy vs pytorch\n",
    "        self.sourcesInNode, self.subset = sources_and_subsets_nodes(self.N_in)\n",
    "        \n",
    "        self.sourcesInNode = [torch.tensor(x) for x in self.sourcesInNode]\n",
    "        self.subset = [torch.tensor(x) for x in self.subset]\n",
    "        \n",
    "    def forward(self,inputs):    \n",
    "        self.FM = self.chi_nn_vars(self.vars)\n",
    "        sortInputs, sortInd = torch.sort(inputs,1, True)\n",
    "        M, N = inputs.size()\n",
    "        sortInputs = torch.cat((sortInputs, torch.zeros(M,1)), 1)\n",
    "        sortInputs = sortInputs[:,:-1] -  sortInputs[:,1:]\n",
    "        \n",
    "        out = torch.cumsum(torch.pow(2,sortInd),1) - torch.ones(1, dtype=torch.int64)\n",
    "        \n",
    "        data = torch.zeros((M,self.nVars+1))\n",
    "        \n",
    "        for i in range(M):\n",
    "            data[i,out[i,:]] = sortInputs[i,:] \n",
    "        \n",
    "        \n",
    "        ChI = torch.matmul(data,self.FM)\n",
    "            \n",
    "        return ChI\n",
    "    \n",
    "    # Converts NN-vars to FM vars\n",
    "    def chi_nn_vars(self, chi_vars):\n",
    "#        nVars,_ = chi_vars.size()\n",
    "        chi_vars = torch.abs(chi_vars)\n",
    "        #        nInputs = inputs.get_shape().as_list()[1]\n",
    "        \n",
    "        FM = chi_vars[None, 0,:]\n",
    "        for i in range(1,self.nVars):\n",
    "            indices = subset_to_indices(self.subset[i])\n",
    "            if (len(indices) == 1):\n",
    "                FM = torch.cat((FM,chi_vars[None,i,:]),0)\n",
    "            else:\n",
    "                #         ss=tf.gather_nd(variables, [[1],[2]])\n",
    "                maxVal,_ = torch.max(FM[indices,:],0)\n",
    "                temp = torch.add(maxVal,chi_vars[i,:])\n",
    "                FM = torch.cat((FM,temp[None,:]),0)\n",
    "              \n",
    "        FM = torch.cat([FM, torch.ones((1,self.N_out))],0)\n",
    "        FM = torch.min(FM, torch.ones(1))  \n",
    "        \n",
    "        return FM\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b443c791",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "# training samples size\n",
    "M = 700\n",
    "\n",
    "# number of inputs\n",
    "N_in = 3\n",
    "\n",
    "# number of outputs aka number of Choquet integral neurons\n",
    "N_out = 2  \n",
    "\n",
    "# Create a synthetic dataset via random sampling from a normal distribution with mean =-1 and std=2\n",
    "X_train = np.random.rand(M,N_in)*2-1\n",
    "\n",
    "# Let's specify the FMs  (There will be N_out number of FMs)\n",
    "# Herein we adopt binary encoding instead of lexicographic encoding to represent a FM that is easier to code. \n",
    "# As for example, an FM for three inputs using lexicographic encoding is, g = {g_1, g_2, g_3, g_{12}, g_{13}, g_{23}, g_{123}}.\n",
    "# whereas its binary encoding is g = {g_1, g_2, g_{12}, g_3 g_{13}, g_{23}, g_{123}}.\n",
    "\n",
    "# For simplicity, here we use OWA. \n",
    "\n",
    "# OWA = np.array([[0.7, 0.2, 0.1], # this is soft-max\n",
    "#                 [0.1,0.2,0.7]])  # soft-min\n",
    "\n",
    "# The FMs of the above OWAs in binary encoding\n",
    "# FM = [[0.7, 0.7, 0.9, 0.7, 0.9, 0.9, 1.0].\n",
    "#      [0.1, 0.1, 0.3, 0.1, 0.3, 0.3, 1.0]]\n",
    "\n",
    "# print('Actual/groundtruth FMs in binary encoding:')\n",
    "# print('FM1 = ', np.array([0.7, 0.7, 0.9, 0.7, 0.9, 0.9, 1.0]))\n",
    "# print('FM2 = ', np.array([0.1, 0.1, 0.3, 0.1, 0.3, 0.3, 1.0]))\n",
    "\n",
    "# Generate the label or the groundtruth based on the provided FMs/OWAs. The labels are two dimentional\n",
    "# label_train = np.matmul(np.sort(X_train), np.fliplr(OWA).T)\n",
    "label_train = np.amax(X_train, 0)\n",
    "\n",
    "# Now we want to recover the FMs from the training data and groundtruth\n",
    "# First, build a Choquet integral neuron with N_in inputs and N_out outputs\n",
    "net = Choquet_integral(N_in,N_out)\n",
    "\n",
    "# set the optimization algorithms and paramters the learning\n",
    "learning_rate = 0.3;\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "criterion = torch.nn.MSELoss(reduction='mean')\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate)   \n",
    "\n",
    "num_epochs = 300;\n",
    "\n",
    "# convert from numpy to torch tensor\n",
    "X_train = torch.tensor(X_train,dtype=torch.float)\n",
    "label_train = torch.tensor(label_train,dtype=torch.float)\n",
    "\n",
    "# optimize\n",
    "for t in range(num_epochs):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = net(X_train)\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = criterion(y_pred, label_train)\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()  \n",
    "\n",
    "# Finally, the learned FMs\n",
    "FM_learned = (net.chi_nn_vars(net.vars).cpu()).detach().numpy()\n",
    "print('\\n\\nLearned FMs:')\n",
    "print('FM1 = ', FM_learned[:,0])\n",
    "print('FM2 = ',FM_learned[:,1])\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
